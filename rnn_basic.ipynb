{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "909163fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e73f466",
   "metadata": {},
   "source": [
    "## RNN Basic\n",
    "### Token\n",
    "For language model, we need to tokenize our words (or characters) into number. We can tokenize our inputs based on each input's frequency. We tokenize our input based on input's frequency because this allow computer to cache more frequently used words and improve training efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9428d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    allowed_characters = string.ascii_letters\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in allowed_characters\n",
    "    )\n",
    "\n",
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self, tokens:List[str], reserved_tokens:List[str]=[], min_freq:int=0) -> None:\n",
    "        self.unk = '<unk>'\n",
    "        self.reserved_tokens = reserved_tokens\n",
    "\n",
    "        counter = Counter(tokens)\n",
    "        tokens_freq = sorted(counter.items(), key=lambda token:token[1], reverse=True)\n",
    "        sorted_tokens = list([self.unk] + self.reserved_tokens + [\n",
    "            token for token, freq in tokens_freq if freq > min_freq\n",
    "        ])\n",
    "\n",
    "        self.token_to_idx = {\n",
    "            token:index for index, token in enumerate(sorted_tokens)\n",
    "        }\n",
    "\n",
    "        self.idx_to_token = {\n",
    "            index:token for index, token, in enumerate(sorted_tokens)\n",
    "        }\n",
    "\n",
    "    def to_idx(self, token:str) -> int:\n",
    "        if token not in self.token_to_idx:\n",
    "            return self.token_to_idx[self.unk]\n",
    "\n",
    "        return self.token_to_idx[token]\n",
    "    \n",
    "    def to_token(self, idx:int) -> str:\n",
    "        if idx not in self.idx_to_token:\n",
    "            return self.unk\n",
    "\n",
    "        return self.idx_to_token[idx]\n",
    "    \n",
    "    def get_most_frequent(self, n: int) -> List[str]:\n",
    "        res = []\n",
    "        start_idx = len(self.unk) + len(self.reserved_tokens)\n",
    "        for idx in range(n):\n",
    "            res.append(self.to_token(start_idx + idx))\n",
    "        return res\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, (list, tuple, slice)):\n",
    "            return [self.__getitem__(i) for i in index]\n",
    "        return self.to_idx(index)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936db287",
   "metadata": {},
   "source": [
    "### Construct Dataset\n",
    "We devided the input text into segments with the length of num_inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c8c61e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, text_file:Path, num_steps:int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        with text_file.open('r') as f:\n",
    "            text = f.read()\n",
    "            splitted_text = text.split()\n",
    "            splitted_text = [self.preprocess_text(text).lower() for text in splitted_text]\n",
    "\n",
    "        self.tokenizer = Tokenizer(splitted_text)\n",
    "        tokenized = [self.tokenizer.to_idx(text) for text in splitted_text]\n",
    "\n",
    "        array = torch.tensor([tokenized[i:i+num_steps+1]\n",
    "                        for i in range(len(tokenized)-num_steps)])\n",
    "        self.X, self.Y = array[:,:-1], array[:,1:]\n",
    "\n",
    "    def preprocess_text(self, text) -> List[str]:\n",
    "        return unicode_to_ascii(text)\n",
    "    \n",
    "    def get_vocab_size(self) -> int:\n",
    "        return len(self.tokenizer)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, (list, tuple)):\n",
    "            return [self.__getitem__(idx) for idx in index]\n",
    "        elif isinstance(index, slice):\n",
    "            return [self.__getitem__(idx) for idx in range(*index.indices(len(self)))]\n",
    "        return (self.X[index], self.Y[index])\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5417588f",
   "metadata": {},
   "source": [
    "### Language Model\n",
    "Let each input in time t denoted as $x_t$. Our goal is to predict $x_t$ given $x_0, x_1, ..., x_{t-1}$.\n",
    "The probability of a sequence of words with length t will then be:\n",
    "$$\n",
    "P(x_1, x_2, ..., x_t) = P(x_1) * \\prod_{t=2}^T P(x_t  \\mid  x_1, \\ldots, x_{t-1})\n",
    "$$\n",
    "\n",
    "The probability of $x_1$ is $P(x_1)$.\n",
    "\n",
    "The probability of $x_1$ and $x_2$ is $P(x_1, x_2) = P(x_2 | x_1) * P(x_1)$.\n",
    "\n",
    "That is, the joint probability of $x_1$ and $x_2$ is just the probability of $x_1$ times the probability of $x_2$ given $x_1$.\n",
    "\n",
    "We can treat the output of the model at each stage as the probability of $x_t$ given $x_1, ..., x_{t-1}$\n",
    "\n",
    "### Perplexity\n",
    "We can measure the cross-entropy loss averaged over all the tokens of a sequence with perplexity:\n",
    "$$\\frac{1}{n} \\sum_{t=1}^n -\\log P(x_t \\mid x_{t-1}, \\ldots, x_1),$$\n",
    "where $P$ is given by a language model and $x_t$ is the actual token observed at time step $t$ from the sequence.\n",
    "This makes the performance on documents of different lengths comparable. For historical reasons, scientists in natural language processing prefer to use a quantity called *perplexity*:\n",
    "\n",
    "$$\\exp\\left(-\\frac{1}{n} \\sum_{t=1}^n \\log P(x_t \\mid x_{t-1}, \\ldots, x_1)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684eeecf",
   "metadata": {},
   "source": [
    "### RNN Model\n",
    "RNN model is similar to MLP. The key difference is that instead of using hidden layer, RNN typically uses hidden states to store all the features from the previous samples. At each batch, the hidden state from the last batch will be multiplied with the current input to compute the current hiddent state. The current hidden state will then be multiplied by a weight to compute the output of the current stage.\n",
    "\n",
    "\n",
    "![screenshot](resources/rnn_with_hidden_state.png)\n",
    "\n",
    "The calculation of the hidden layer output of the current time step is determined by the input of the current time step together with the hidden layer output of the previous time step:\n",
    "\n",
    "$$\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{\\textrm{xh}} + \\mathbf{H}_{t-1} \\mathbf{W}_{\\textrm{hh}}  + \\mathbf{b}_\\textrm{h}).$$\n",
    "\n",
    "For time step $t$,\n",
    "the output of the output layer is similar to the computation in the MLP:\n",
    "\n",
    "$$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{\\textrm{hq}} + \\mathbf{b}_\\textrm{q}.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d9854c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Scratch(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs:int, num_hiddens:int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "\n",
    "        self.w_xh = nn.Parameter(\n",
    "            torch.randn(num_inputs, num_hiddens)\n",
    "        )\n",
    "\n",
    "        self.w_hh = nn.Parameter(\n",
    "            torch.randn((num_hiddens, num_hiddens))\n",
    "        )\n",
    "\n",
    "        self.b_h = nn.Parameter(\n",
    "            torch.zeros(num_hiddens)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X, state=None):\n",
    "        if state is None:\n",
    "            state = torch.zeros((X.shape[1], self.num_hiddens), device=X.device)\n",
    "        else:\n",
    "            state, = state\n",
    "\n",
    "        # Input size will be (steps, batchs, inputs)\n",
    "        outputs = []\n",
    "        for step in X:\n",
    "            cur_hidden = torch.matmul(step, self.w_xh) + torch.matmul(state, self.w_hh) + self.b_h\n",
    "            cur_hidden = torch.tanh(cur_hidden)\n",
    "            outputs.append(cur_hidden)\n",
    "\n",
    "        return outputs, state\n",
    "\n",
    "class RNN_LM_Scratch(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn: RNN_Scratch, num_vocabs:int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn = rnn\n",
    "        self.num_vocabs = num_vocabs\n",
    "        \n",
    "        self.w_hq = nn.Parameter(\n",
    "            torch.randn((rnn.num_hiddens, self.num_vocabs))\n",
    "        )\n",
    "\n",
    "        self.b_q = nn.Parameter(\n",
    "            torch.zeros(self.num_vocabs)\n",
    "        )\n",
    "\n",
    "    def forward(self, X, state=None):\n",
    "        \n",
    "        # The shape of the embedding will be (step, batch, input)\n",
    "        emb = F.one_hot(X.T, self.num_vocabs).type(torch.float32)\n",
    "        \n",
    "        # The shape of the outputs will be (steps, hidden)\n",
    "        rnn_outputs, _ = self.rnn(emb, state)\n",
    "\n",
    "        # each output will have the dimension of (batch, output).\n",
    "        # we stack at dim=1 so that the output will have size of (step, batch, output)\n",
    "        outputs = [torch.matmul(hidden_state, self.w_hq) + self.b_q for hidden_state in rnn_outputs]\n",
    "        return torch.stack(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "785a5545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14045, 3512, 3182)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = d2l.TimeMachine(batch_size=1024, num_steps=32)\n",
    "# rnn = RNNScratch(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "# model = RNNLMScratch(rnn, vocab_size=len(data.vocab), lr=1)\n",
    "# trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)\n",
    "# trainer.fit(model, data)\n",
    "\n",
    "batch_size, num_hiddens, num_steps = 1024, 32, 32\n",
    "\n",
    "dataset = TextDataset(Path(r'shakespeare.txt'), num_steps)\n",
    "train_num = int(len(dataset) * 0.8)\n",
    "train_dataset = dataset[:train_num]\n",
    "valid_dataset = dataset[train_num:]\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "test_loader = DataLoader(dataset=valid_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False,\n",
    "                         num_workers=True)\n",
    "\n",
    "len(train_dataset), len(valid_dataset), dataset.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfe297fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model: nn.Module, train_loader: DataLoader, loss_func, optim, device=torch.device('cpu')):\n",
    "\n",
    "    losses = []\n",
    "    model.train()\n",
    "\n",
    "    for i, (X, Y) in enumerate(train_loader):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        Y = F.one_hot(Y, dataset.get_vocab_size()).type(torch.float32)\n",
    "\n",
    "        predict = model(X)\n",
    "        loss = loss_func(predict, Y)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return sum(losses) / len(losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbaa64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "1: loss: 0.07840935992343086\n",
      "2: loss: 0.07836670162422317\n",
      "3: loss: 0.07833310855286461\n",
      "4: loss: 0.07829061789172036\n",
      "5: loss: 0.07825702535254615\n",
      "6: loss: 0.07821500354579516\n",
      "7: loss: 0.07816796962703977\n",
      "8: loss: 0.07815078858818326\n",
      "9: loss: 0.07810440819178309\n",
      "10: loss: 0.07806458643504552\n",
      "11: loss: 0.07804071796791894\n",
      "12: loss: 0.07798616268805095\n",
      "13: loss: 0.07795831667525428\n",
      "14: loss: 0.07792265021375247\n",
      "15: loss: 0.07787770511848587\n",
      "16: loss: 0.07784160279801913\n",
      "17: loss: 0.07780767551490239\n",
      "18: loss: 0.07777572689311844\n",
      "19: loss: 0.07773340972406524\n",
      "20: loss: 0.07769589232546943\n",
      "21: loss: 0.07765578106045723\n",
      "22: loss: 0.07761431857943535\n",
      "23: loss: 0.07757577672600746\n",
      "24: loss: 0.07753164108310427\n",
      "25: loss: 0.07749998569488525\n",
      "26: loss: 0.07746076796736036\n",
      "27: loss: 0.07743452436157636\n",
      "28: loss: 0.07739413689289774\n",
      "29: loss: 0.07734253257513046\n",
      "30: loss: 0.07731079842363085\n",
      "31: loss: 0.07727484724351338\n",
      "32: loss: 0.07723814089383398\n",
      "33: loss: 0.07720021424548966\n",
      "34: loss: 0.07715745163815362\n",
      "35: loss: 0.07713231550795692\n",
      "36: loss: 0.07708145731261798\n",
      "37: loss: 0.07704514903681618\n",
      "38: loss: 0.07701385819486209\n",
      "39: loss: 0.07696535757609776\n",
      "40: loss: 0.07693126797676086\n",
      "41: loss: 0.07688799926212855\n",
      "42: loss: 0.07684950157999992\n",
      "43: loss: 0.0768186327602182\n",
      "44: loss: 0.07678262197545596\n",
      "45: loss: 0.07673831496919904\n",
      "46: loss: 0.07669044179575783\n",
      "47: loss: 0.07666432591421264\n",
      "48: loss: 0.07661962189844676\n",
      "49: loss: 0.07657789971147265\n",
      "50: loss: 0.07654609211853572\n",
      "51: loss: 0.0765005009514945\n",
      "52: loss: 0.0764625636594636\n",
      "53: loss: 0.07643116265535355\n",
      "54: loss: 0.07639854507786888\n",
      "55: loss: 0.07634796148964337\n",
      "56: loss: 0.07630652615002223\n",
      "57: loss: 0.07626669108867645\n",
      "58: loss: 0.07623173668980598\n",
      "59: loss: 0.07618882347430501\n",
      "60: loss: 0.0761531229530062\n",
      "61: loss: 0.07610892930201121\n",
      "62: loss: 0.07606933957764081\n",
      "63: loss: 0.07601838825004441\n",
      "64: loss: 0.07598521028246198\n",
      "65: loss: 0.0759399916444506\n",
      "66: loss: 0.07590218101228986\n",
      "67: loss: 0.07586102134415082\n",
      "68: loss: 0.07582795087780271\n",
      "69: loss: 0.07576739362307958\n",
      "70: loss: 0.07573305921895164\n",
      "71: loss: 0.07569386810064316\n",
      "72: loss: 0.07565025559493474\n",
      "73: loss: 0.07560744881629944\n",
      "74: loss: 0.07557162695697375\n",
      "75: loss: 0.0755195575101035\n",
      "76: loss: 0.07548773235508374\n",
      "77: loss: 0.07544163508074624\n",
      "78: loss: 0.07539107118334089\n",
      "79: loss: 0.07535598373838834\n",
      "80: loss: 0.07531427964568138\n",
      "81: loss: 0.07526534689324242\n",
      "82: loss: 0.07521956147892135\n",
      "83: loss: 0.07517176121473312\n",
      "84: loss: 0.07512354105710983\n",
      "85: loss: 0.07508308653320585\n",
      "86: loss: 0.07503734582236835\n",
      "87: loss: 0.07498426149998393\n",
      "88: loss: 0.07494516564267022\n",
      "89: loss: 0.07489247832979475\n",
      "90: loss: 0.07486050203442574\n",
      "91: loss: 0.07481112969773156\n",
      "92: loss: 0.07476071640849113\n",
      "93: loss: 0.07471274957060814\n",
      "94: loss: 0.07466352943863187\n",
      "95: loss: 0.07460633984633855\n",
      "96: loss: 0.07456762503300395\n",
      "97: loss: 0.07451345611895833\n",
      "98: loss: 0.0744643041065761\n",
      "99: loss: 0.07441974165184158\n",
      "100: loss: 0.07437460922769137\n",
      "101: loss: 0.07431848613279206\n",
      "102: loss: 0.07426023110747337\n",
      "103: loss: 0.07421093966279711\n",
      "104: loss: 0.07415384267057691\n",
      "105: loss: 0.07411491391914231\n",
      "106: loss: 0.07405510225466319\n",
      "107: loss: 0.07400866544672421\n",
      "108: loss: 0.07395830644028527\n",
      "109: loss: 0.07390223816037178\n",
      "110: loss: 0.07384485325642995\n",
      "111: loss: 0.07378718523042542\n",
      "112: loss: 0.07374042432223048\n",
      "113: loss: 0.07367466709443501\n",
      "114: loss: 0.0736171794789178\n",
      "115: loss: 0.07356380405170578\n",
      "116: loss: 0.07350813171693257\n",
      "117: loss: 0.07344914972782135\n",
      "118: loss: 0.07338847219944\n",
      "119: loss: 0.07333813501255852\n",
      "120: loss: 0.07327172426240784\n",
      "121: loss: 0.07322071173361369\n",
      "122: loss: 0.07316052221826144\n",
      "123: loss: 0.07310214638710022\n",
      "124: loss: 0.07304164554391589\n",
      "125: loss: 0.07297484736357417\n",
      "126: loss: 0.07291073245661599\n",
      "127: loss: 0.0728558392396995\n",
      "128: loss: 0.07278648125273841\n",
      "129: loss: 0.07272322263036456\n",
      "130: loss: 0.07265972133193697\n",
      "131: loss: 0.07259345852902957\n",
      "132: loss: 0.07253898360899516\n",
      "133: loss: 0.07246925096426692\n",
      "134: loss: 0.07239901168005806\n",
      "135: loss: 0.07232509659869331\n",
      "136: loss: 0.07226122756089483\n",
      "137: loss: 0.07218929646270615\n",
      "138: loss: 0.07212479572210993\n",
      "139: loss: 0.07205251870410782\n",
      "140: loss: 0.07198960706591606\n",
      "141: loss: 0.07190306804009847\n",
      "142: loss: 0.0718374252319336\n",
      "143: loss: 0.07177001397524561\n",
      "144: loss: 0.07170299600277628\n",
      "145: loss: 0.07163109311035701\n",
      "146: loss: 0.07154197831239019\n",
      "147: loss: 0.07147370757801193\n",
      "148: loss: 0.07140304786818367\n",
      "149: loss: 0.07131940073200635\n",
      "150: loss: 0.07123978329556328\n",
      "151: loss: 0.0711680996630873\n",
      "152: loss: 0.0710891458605017\n",
      "153: loss: 0.0710091202386788\n",
      "154: loss: 0.07093333985124316\n",
      "155: loss: 0.07085233447807175\n",
      "156: loss: 0.0707664814378534\n",
      "157: loss: 0.0706866809300014\n",
      "158: loss: 0.07060796075633594\n",
      "159: loss: 0.07052907666989736\n",
      "160: loss: 0.07044584623404912\n",
      "161: loss: 0.07035637221166066\n",
      "162: loss: 0.07027786118643624\n",
      "163: loss: 0.07018997786300522\n",
      "164: loss: 0.07009955495595932\n",
      "165: loss: 0.07001553422638349\n",
      "166: loss: 0.06993215318237032\n",
      "167: loss: 0.06984630706054824\n",
      "168: loss: 0.06974830371992928\n",
      "169: loss: 0.0696652994624206\n",
      "170: loss: 0.06957655293600899\n",
      "171: loss: 0.06948180390255791\n",
      "172: loss: 0.06939095950552396\n",
      "173: loss: 0.06929914546864373\n",
      "174: loss: 0.06920457950660161\n",
      "175: loss: 0.06911095976829529\n",
      "176: loss: 0.06901972421577998\n",
      "177: loss: 0.06892973184585571\n",
      "178: loss: 0.06883502591933523\n",
      "179: loss: 0.06873166561126709\n",
      "180: loss: 0.06864043165530477\n",
      "181: loss: 0.06853885097163064\n",
      "182: loss: 0.06845015340617724\n",
      "183: loss: 0.06834823478545461\n",
      "184: loss: 0.06824918144515582\n",
      "185: loss: 0.06815311099801745\n",
      "186: loss: 0.06804893644792694\n",
      "187: loss: 0.06795029288956098\n",
      "188: loss: 0.06784983937229429\n",
      "189: loss: 0.06775217556527682\n",
      "190: loss: 0.06764708938343185\n",
      "191: loss: 0.06755200454166957\n",
      "192: loss: 0.06744002550840378\n",
      "193: loss: 0.06733519211411476\n",
      "194: loss: 0.06723615261060852\n",
      "195: loss: 0.06713389552065305\n",
      "196: loss: 0.0670177952519485\n",
      "197: loss: 0.06691545460905347\n",
      "198: loss: 0.06680763140320778\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'device: {device}')\n",
    "\n",
    "rnn = RNN_Scratch(num_inputs=dataset.get_vocab_size(),\n",
    "                  num_hiddens=num_hiddens)\n",
    "model = RNN_LM_Scratch(rnn=rnn,\n",
    "                       num_vocabs=dataset.get_vocab_size())\n",
    "\n",
    "rnn = rnn.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = train_loop(model, train_loader, loss, optim, device=device)\n",
    "    print(f'{epoch+1}: loss: {avg_loss}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

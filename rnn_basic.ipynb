{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "909163fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\howar\\OneDrive\\桌面\\Code\\Python\\RNN_Basic\\.venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e73f466",
   "metadata": {},
   "source": [
    "## RNN Basic\n",
    "### Token\n",
    "For language model, we need to tokenize our words (or characters) into number. We can tokenize our inputs based on each input's frequency. We tokenize our input based on input's frequency because this allow computer to cache more frequently used words and improve training efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9428d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    allowed_characters = string.ascii_letters\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in allowed_characters\n",
    "    )\n",
    "\n",
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self, tokens:List[str], reserved_tokens:List[str]=[], min_freq:int=0) -> None:\n",
    "        self.unk = '<unk>'\n",
    "        self.reserved_tokens = reserved_tokens\n",
    "\n",
    "        counter = Counter(tokens)\n",
    "        tokens_freq = sorted(counter.items(), key=lambda token:token[1], reverse=True)\n",
    "        sorted_tokens = list([self.unk] + self.reserved_tokens + [\n",
    "            token for token, freq in tokens_freq if freq > min_freq\n",
    "        ])\n",
    "\n",
    "        self.token_to_idx = {\n",
    "            token:index for index, token in enumerate(sorted_tokens)\n",
    "        }\n",
    "\n",
    "        self.idx_to_token = {\n",
    "            index:token for index, token, in enumerate(sorted_tokens)\n",
    "        }\n",
    "\n",
    "    def to_idx(self, token:str) -> int:\n",
    "        if token not in self.token_to_idx:\n",
    "            return self.token_to_idx[self.unk]\n",
    "\n",
    "        return self.token_to_idx[token]\n",
    "    \n",
    "    def to_token(self, idx:int) -> str:\n",
    "        if idx not in self.idx_to_token:\n",
    "            return self.unk\n",
    "\n",
    "        return self.idx_to_token[idx]\n",
    "    \n",
    "    def get_most_frequent(self, n: int) -> List[str]:\n",
    "        res = []\n",
    "        start_idx = len(self.unk) + len(self.reserved_tokens)\n",
    "        for idx in range(n):\n",
    "            res.append(self.to_token(start_idx + idx))\n",
    "        return res\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, (list, tuple, slice)):\n",
    "            return [self.__getitem__(i) for i in index]\n",
    "        return self.to_idx(index)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936db287",
   "metadata": {},
   "source": [
    "### Construct Dataset\n",
    "We devided the input text into segments with the length of num_inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c8c61e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, text_file:Path, num_steps:int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        with text_file.open('r') as f:\n",
    "            text = f.read()\n",
    "            splitted_text = text.split()\n",
    "            splitted_text = [self.preprocess_text(text).lower() for text in splitted_text]\n",
    "\n",
    "        self.tokenizer = Tokenizer(splitted_text)\n",
    "        tokenized = [self.tokenizer.to_idx(text) for text in splitted_text]\n",
    "\n",
    "        array = torch.tensor([tokenized[i:i+num_steps+1]\n",
    "                        for i in range(len(tokenized)-num_steps)])\n",
    "        self.X, self.Y = array[:,:-1], array[:,1:]\n",
    "\n",
    "    def preprocess_text(self, text) -> List[str]:\n",
    "        return unicode_to_ascii(text)\n",
    "    \n",
    "    def get_vocab_size(self) -> int:\n",
    "        return len(self.tokenizer)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, (list, tuple)):\n",
    "            return [self.__getitem__(idx) for idx in index]\n",
    "        elif isinstance(index, slice):\n",
    "            return [self.__getitem__(idx) for idx in range(*index.indices(len(self)))]\n",
    "        return (self.X[index], self.Y[index])\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5417588f",
   "metadata": {},
   "source": [
    "### Language Model\n",
    "Let each input in time t denoted as $x_t$. Our goal is to predict $x_t$ given $x_0, x_1, ..., x_{t-1}$.\n",
    "The probability of a sequence of words with length t will then be:\n",
    "$$\n",
    "P(x_1, x_2, ..., x_t) = P(x_1) * \\prod_{t=2}^T P(x_t  \\mid  x_1, \\ldots, x_{t-1})\n",
    "$$\n",
    "\n",
    "The probability of $x_1$ is $P(x_1)$.\n",
    "\n",
    "The probability of $x_1$ and $x_2$ is $P(x_1, x_2) = P(x_2 | x_1) * P(x_1)$.\n",
    "\n",
    "That is, the joint probability of $x_1$ and $x_2$ is just the probability of $x_1$ times the probability of $x_2$ given $x_1$.\n",
    "\n",
    "We can treat the output of the model at each stage as the probability of $x_t$ given $x_1, ..., x_{t-1}$\n",
    "\n",
    "### Perplexity\n",
    "We can measure the cross-entropy loss averaged over all the tokens of a sequence with perplexity:\n",
    "$$\\frac{1}{n} \\sum_{t=1}^n -\\log P(x_t \\mid x_{t-1}, \\ldots, x_1),$$\n",
    "where $P$ is given by a language model and $x_t$ is the actual token observed at time step $t$ from the sequence.\n",
    "This makes the performance on documents of different lengths comparable. For historical reasons, scientists in natural language processing prefer to use a quantity called *perplexity*:\n",
    "\n",
    "$$\\exp\\left(-\\frac{1}{n} \\sum_{t=1}^n \\log P(x_t \\mid x_{t-1}, \\ldots, x_1)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684eeecf",
   "metadata": {},
   "source": [
    "### RNN Model\n",
    "RNN model is similar to MLP. The key difference is that instead of using hidden layer, RNN typically uses hidden states to store all the features from the previous samples. At each batch, the hidden state from the last batch will be multiplied with the current input to compute the current hiddent state. The current hidden state will then be multiplied by a weight to compute the output of the current stage.\n",
    "\n",
    "\n",
    "![screenshot](resources/rnn_with_hidden_state.png)\n",
    "\n",
    "The calculation of the hidden layer output of the current time step is determined by the input of the current time step together with the hidden layer output of the previous time step:\n",
    "\n",
    "$$\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{\\textrm{xh}} + \\mathbf{H}_{t-1} \\mathbf{W}_{\\textrm{hh}}  + \\mathbf{b}_\\textrm{h}).$$\n",
    "\n",
    "For time step $t$,\n",
    "the output of the output layer is similar to the computation in the MLP:\n",
    "\n",
    "$$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{\\textrm{hq}} + \\mathbf{b}_\\textrm{q}.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d9854c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0), tensor(0))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN_Scratch(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs:int, num_hiddens:int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "\n",
    "        self.w_xh = nn.Parameter(\n",
    "            torch.randn(num_inputs, num_hiddens)\n",
    "        )\n",
    "\n",
    "        self.b_xh = nn.Parameter(\n",
    "            torch.randn(num_hiddens)\n",
    "        )\n",
    "\n",
    "        self.w_hh = nn.Parameter(\n",
    "            torch.randn((num_hiddens, num_hiddens))\n",
    "        )\n",
    "\n",
    "        self.b_hh = nn.Parameter(\n",
    "            torch.zeros(num_hiddens)\n",
    "        )\n",
    "\n",
    "    def forward(self, X, state=None):\n",
    "        if state is None:\n",
    "            state = torch.zeros((self.num_hiddens, self.num_hiddens))\n",
    "\n",
    "        # Input size will be (steps, batchs, inputs)\n",
    "        outputs = []\n",
    "        h_t = state\n",
    "        h_t_minus_1 = state\n",
    "\n",
    "        for step in X:\n",
    "            h_t = F.tanh(\n",
    "                step @ self.w_xh.T\n",
    "                + self.b_xh\n",
    "                + h_t_minus_1 @ self.w_hh.T\n",
    "                + self.b_hh\n",
    "            )\n",
    "            outputs.append(h_t.squeeze())\n",
    "            h_t_minus_1 = h_t\n",
    "\n",
    "        outputs = torch.stack(outputs)\n",
    "        return outputs, h_t\n",
    "\n",
    "class RNN_LM_Scratch(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn: RNN_Scratch, num_vocabs:int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn = rnn\n",
    "        self.num_vocabs = num_vocabs\n",
    "        \n",
    "        self.w_hq = nn.Parameter(\n",
    "            torch.randn((rnn.num_hiddens, self.num_vocabs))\n",
    "        )\n",
    "\n",
    "        self.b_q = nn.Parameter(\n",
    "            torch.zeros(self.num_vocabs)\n",
    "        )\n",
    "\n",
    "    def forward(self, X, state=None):\n",
    "        \n",
    "        # The shape of the embedding will be (step, batch, input)\n",
    "        emb = F.one_hot(X.T, self.num_vocabs).type(torch.float32)\n",
    "        \n",
    "        # The shape of the outputs will be (steps, hidden)\n",
    "        rnn_outputs, _ = self.rnn(emb, state)\n",
    "\n",
    "        # each output will have the dimension of (batch, output).\n",
    "        # we stack at dim=1 so that the output will have size of (step, batch, output)\n",
    "        outputs = [torch.matmul(hidden_state, self.w_hq) + self.b_q for hidden_state in rnn_outputs]\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "# Declare RNN from PyTorch and Scratch.\n",
    "rnn = nn.RNN(10, 20, 1)\n",
    "rnn_scratch = RNN_Scratch(10, 20)\n",
    "\n",
    "# Let RNNs share the same weight.\n",
    "rnn_scratch.w_xh = rnn.weight_ih_l0\n",
    "rnn_scratch.b_xh = rnn.bias_ih_l0\n",
    "rnn_scratch.w_hh = rnn.weight_hh_l0\n",
    "rnn_scratch.b_hh = rnn.bias_hh_l0\n",
    "\n",
    "# Sequence, Batch, Input\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(1, 3, 20)\n",
    "output, hn = rnn(input, h0)\n",
    "output_scratch, hn_scratch = rnn_scratch(input, h0)\n",
    "\n",
    "# Compare output\n",
    "compare_func = lambda a, b, threshold: torch.abs(a - b) > threshold\n",
    "compare_func(output, output_scratch, 0.0001).sum(), compare_func(hn, hn_scratch, 0.0001).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b18c8b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14045, 3512, 3182)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = d2l.TimeMachine(batch_size=1024, num_steps=32)\n",
    "# rnn = RNNScratch(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "# model = RNNLMScratch(rnn, vocab_size=len(data.vocab), lr=1)\n",
    "# trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)\n",
    "# trainer.fit(model, data)\n",
    "\n",
    "batch_size, num_hiddens, num_steps = 1024, 32, 32\n",
    "\n",
    "dataset = TextDataset(Path(r'shakespeare.txt'), num_steps)\n",
    "train_num = int(len(dataset) * 0.8)\n",
    "train_dataset = dataset[:train_num]\n",
    "valid_dataset = dataset[train_num:]\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "test_loader = DataLoader(dataset=valid_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False,\n",
    "                         num_workers=True)\n",
    "\n",
    "len(train_dataset), len(valid_dataset), dataset.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45565c37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfe297fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model: nn.Module, train_loader: DataLoader, loss_func, optim, device=torch.device('cpu')):\n",
    "\n",
    "    losses = []\n",
    "    model.train()\n",
    "\n",
    "    for i, (X, Y) in enumerate(train_loader):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        Y = F.one_hot(Y, dataset.get_vocab_size()).type(torch.float32)\n",
    "\n",
    "        predict = model(X)\n",
    "        loss = loss_func(predict, Y)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return sum(losses) / len(losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adbaa64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1024x3182 and 32x3182)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m num_epochs = \u001b[32m200\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     avg_loss = \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mtrain_loop\u001b[39m\u001b[34m(model, train_loader, loss_func, optim, device)\u001b[39m\n\u001b[32m      7\u001b[39m X, Y = X.to(device), Y.to(device)\n\u001b[32m      8\u001b[39m Y = F.one_hot(Y, dataset.get_vocab_size()).type(torch.float32)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m predict = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m loss = loss_func(predict, Y)\n\u001b[32m     13\u001b[39m optim.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\howar\\OneDrive\\桌面\\Code\\Python\\RNN_Basic\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\howar\\OneDrive\\桌面\\Code\\Python\\RNN_Basic\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mRNN_LM_Scratch.forward\u001b[39m\u001b[34m(self, X, state)\u001b[39m\n\u001b[32m     66\u001b[39m emb = F.one_hot(X.T, \u001b[38;5;28mself\u001b[39m.num_vocabs).type(torch.float32)\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# The shape of the outputs will be (steps, hidden)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m rnn_outputs, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# each output will have the dimension of (batch, output).\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# we stack at dim=1 so that the output will have size of (step, batch, output)\u001b[39;00m\n\u001b[32m     73\u001b[39m outputs = [torch.matmul(hidden_state, \u001b[38;5;28mself\u001b[39m.w_hq) + \u001b[38;5;28mself\u001b[39m.b_q \u001b[38;5;28;01mfor\u001b[39;00m hidden_state \u001b[38;5;129;01min\u001b[39;00m rnn_outputs]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\howar\\OneDrive\\桌面\\Code\\Python\\RNN_Basic\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\howar\\OneDrive\\桌面\\Code\\Python\\RNN_Basic\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mRNN_Scratch.forward\u001b[39m\u001b[34m(self, X, state)\u001b[39m\n\u001b[32m     32\u001b[39m h_t_minus_1 = state\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m X:\n\u001b[32m     35\u001b[39m     h_t = F.tanh(\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         \u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mw_xh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\n\u001b[32m     37\u001b[39m         + \u001b[38;5;28mself\u001b[39m.b_xh\n\u001b[32m     38\u001b[39m         + h_t_minus_1 @ \u001b[38;5;28mself\u001b[39m.w_hh.T\n\u001b[32m     39\u001b[39m         + \u001b[38;5;28mself\u001b[39m.b_hh\n\u001b[32m     40\u001b[39m     )\n\u001b[32m     41\u001b[39m     outputs.append(h_t.squeeze())\n\u001b[32m     42\u001b[39m     h_t_minus_1 = h_t\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (1024x3182 and 32x3182)"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'device: {device}')\n",
    "\n",
    "rnn = RNN_Scratch(num_inputs=dataset.get_vocab_size(),\n",
    "                  num_hiddens=num_hiddens)\n",
    "model = RNN_LM_Scratch(rnn=rnn,\n",
    "                       num_vocabs=dataset.get_vocab_size())\n",
    "\n",
    "rnn = rnn.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = train_loop(model, train_loader, loss, optim, device=device)\n",
    "    print(f'{epoch+1}: loss: {avg_loss}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
